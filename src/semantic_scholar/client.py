"""Async HTTP client for Semantic Scholar API with rate limiting."""

import asyncio
import logging
import time
from typing import Any

import httpx

from ..config import (
    SEMANTIC_SCHOLAR_API_KEY,
    SEMANTIC_SCHOLAR_BASE_URL,
    RATE_LIMIT_REQUESTS_PER_SECOND,
    RATE_LIMIT_REQUESTS_PER_SECOND_NO_KEY,
    MAX_RETRIES,
    RETRY_BACKOFF_FACTOR,
)

logger = logging.getLogger(__name__)


class RateLimiter:
    """Token bucket rate limiter."""

    def __init__(self, requests_per_second: float):
        self.requests_per_second = requests_per_second
        self.min_interval = 1.0 / requests_per_second
        self.last_request_time = 0.0
        self._lock = asyncio.Lock()

    async def acquire(self) -> None:
        """Wait until we can make another request."""
        async with self._lock:
            now = time.monotonic()
            time_since_last = now - self.last_request_time
            if time_since_last < self.min_interval:
                await asyncio.sleep(self.min_interval - time_since_last)
            self.last_request_time = time.monotonic()


class SemanticScholarClient:
    """Async client for Semantic Scholar API."""

    def __init__(self, api_key: str | None = None):
        self.api_key = api_key or SEMANTIC_SCHOLAR_API_KEY
        self.base_url = SEMANTIC_SCHOLAR_BASE_URL

        # Set rate limit based on whether we have an API key
        rate_limit = (
            RATE_LIMIT_REQUESTS_PER_SECOND
            if self.api_key
            else RATE_LIMIT_REQUESTS_PER_SECOND_NO_KEY
        )
        self.rate_limiter = RateLimiter(rate_limit)

        # Build headers
        self.headers: dict[str, str] = {}
        if self.api_key:
            self.headers["x-api-key"] = self.api_key
            logger.info("Semantic Scholar client initialized with API key")
        else:
            logger.warning("No API key provided - rate limiting will be strict")

        self._client: httpx.AsyncClient | None = None

    async def __aenter__(self) -> "SemanticScholarClient":
        self._client = httpx.AsyncClient(
            base_url=self.base_url,
            headers=self.headers,
            timeout=30.0,
        )
        return self

    async def __aexit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        if self._client:
            await self._client.aclose()
            self._client = None

    @property
    def client(self) -> httpx.AsyncClient:
        if self._client is None:
            raise RuntimeError(
                "Client not initialized. Use 'async with' context manager."
            )
        return self._client

    async def _request_with_retry(
        self,
        method: str,
        url: str,
        **kwargs: Any,
    ) -> httpx.Response:
        """Make a request with rate limiting and exponential backoff retry."""
        last_exception: Exception | None = None
        last_response: httpx.Response | None = None

        for attempt in range(MAX_RETRIES):
            await self.rate_limiter.acquire()
            logger.debug(f"Request attempt {attempt + 1}/{MAX_RETRIES}: {method} {url}")

            try:
                response = await self.client.request(method, url, **kwargs)
                last_response = response
                logger.debug(f"Response status: {response.status_code}")

                # Handle rate limiting (429) with exponential backoff
                if response.status_code == 429:
                    retry_after = int(response.headers.get("Retry-After", 0))
                    # Exponential backoff: 10s, 20s, 40s, 80s, 160s
                    backoff = max(retry_after, 10 * (2 ** attempt))
                    logger.warning(f"Rate limited (429), waiting {backoff}s (attempt {attempt + 1})")
                    await asyncio.sleep(backoff)
                    continue

                # Handle server errors with retry
                if response.status_code in (500, 502, 503, 504):
                    backoff = RETRY_BACKOFF_FACTOR ** attempt
                    logger.warning(f"Server error ({response.status_code}), backoff {backoff}s")
                    await asyncio.sleep(backoff)
                    continue

                response.raise_for_status()
                return response

            except httpx.HTTPStatusError as e:
                last_exception = e
                logger.error(f"HTTP error: {e.response.status_code} - {e.response.text[:200]}")
                if e.response.status_code in (429, 500, 502, 503, 504):
                    backoff = RETRY_BACKOFF_FACTOR ** attempt
                    await asyncio.sleep(backoff)
                    continue
                raise

            except (httpx.ConnectError, httpx.ReadTimeout) as e:
                last_exception = e
                backoff = RETRY_BACKOFF_FACTOR ** attempt
                logger.warning(f"Connection error: {e}, backoff {backoff}s")
                await asyncio.sleep(backoff)
                continue

        # Provide better error message
        logger.error(f"Request failed after {MAX_RETRIES} retries")
        if last_exception:
            raise last_exception
        if last_response is not None:
            raise httpx.HTTPStatusError(
                f"Request failed with status {last_response.status_code}: {last_response.text}",
                request=last_response.request,
                response=last_response,
            )
        raise RuntimeError("Request failed after all retries")

    async def get(self, url: str, **kwargs: Any) -> httpx.Response:
        """Make a GET request."""
        return await self._request_with_retry("GET", url, **kwargs)

    async def post(self, url: str, **kwargs: Any) -> httpx.Response:
        """Make a POST request."""
        return await self._request_with_retry("POST", url, **kwargs)

    async def search_papers(
        self,
        query: str,
        fields: list[str] | None = None,
        limit: int = 100,
        offset: int = 0,
        **filter_params: str,
    ) -> dict[str, Any]:
        """Search for papers using the /paper/search endpoint."""
        default_fields = [
            "paperId",
            "title",
            "abstract",
            "authors",
            "year",
            "citationCount",
            "fieldsOfStudy",
            "publicationTypes",
        ]

        params: dict[str, Any] = {
            "query": query,
            "fields": ",".join(fields or default_fields),
            "limit": min(limit, 100),  # API max is 100 per request
            "offset": offset,
        }
        params.update(filter_params)

        logger.info(f"Searching papers: query='{query}', limit={limit}, offset={offset}")
        logger.debug(f"Filter params: {filter_params}")

        response = await self.get("/paper/search", params=params)
        data = response.json()

        total = data.get("total", 0)
        found = len(data.get("data", []))
        logger.info(f"Search returned {found} papers (total available: {total})")

        return data

    async def get_paper_batch(
        self,
        paper_ids: list[str],
        fields: list[str] | None = None,
    ) -> list[dict[str, Any]]:
        """Fetch multiple papers by ID using the /paper/batch endpoint."""
        default_fields = [
            "paperId",
            "title",
            "abstract",
            "authors",
            "year",
            "citationCount",
            "fieldsOfStudy",
            "publicationTypes",
            "openAccessPdf",
            "venue",
            "url",
            "externalIds",
        ]

        params = {"fields": ",".join(fields or default_fields)}

        response = await self.post(
            "/paper/batch",
            params=params,
            json={"ids": paper_ids},
        )
        return response.json()

    async def download_pdf(self, url: str) -> bytes:
        """Download a PDF from a URL."""
        # Create a separate client for PDF downloads (different base URL)
        async with httpx.AsyncClient(timeout=60.0, follow_redirects=True) as client:
            response = await client.get(url)
            response.raise_for_status()
            return response.content
