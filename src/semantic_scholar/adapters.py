"""Adapter implementations for the paper search protocols."""

import logging
import tempfile
from pathlib import Path

from .client import SemanticScholarClient

logger = logging.getLogger(__name__)
from .models import (
    PaperSearchResult,
    PaperDetails,
    SearchFilters,
    SearchResponse,
)
from .protocols import PaperSearchProvider, PDFExtractor


class SemanticScholarAdapter(PaperSearchProvider, PDFExtractor):
    """
    Adapter for Semantic Scholar API.

    Implements both PaperSearchProvider and PDFExtractor protocols.

    Usage:
        async with SemanticScholarAdapter() as adapter:
            results = await adapter.search_papers("machine learning")
            details = await adapter.fetch_papers([r.paper_id for r in results])
    """

    def __init__(self, api_key: str | None = None):
        """
        Initialize the Semantic Scholar adapter.

        Args:
            api_key: Optional API key. If not provided, uses SEMANTIC_SCHOLAR_API_KEY
                    environment variable.
        """
        self._client = SemanticScholarClient(api_key=api_key)
        self._entered = False

    async def __aenter__(self) -> "SemanticScholarAdapter":
        await self._client.__aenter__()
        self._entered = True
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:
        await self._client.__aexit__(exc_type, exc_val, exc_tb)
        self._entered = False

    def _ensure_entered(self) -> None:
        if not self._entered:
            raise RuntimeError(
                "Adapter not initialized. Use 'async with' context manager."
            )

    async def search_papers(
        self,
        query: str,
        filters: SearchFilters | None = None,
        limit: int = 100,
    ) -> list[PaperSearchResult]:
        """
        Search for papers matching query and filters.

        Uses Semantic Scholar /paper/search endpoint.
        """
        self._ensure_entered()
        filters = filters or SearchFilters()
        filter_params = filters.to_query_params()

        all_results: list[PaperSearchResult] = []
        offset = 0

        while len(all_results) < limit:
            remaining = limit - len(all_results)
            batch_size = min(remaining, 100)  # API max is 100

            response_data = await self._client.search_papers(
                query=query,
                limit=batch_size,
                offset=offset,
                **filter_params,
            )

            response = SearchResponse.model_validate(response_data)

            for paper_data in response.data:
                all_results.append(paper_data)

            # Check if there are more results
            if response.next is None or len(response.data) == 0:
                break

            offset = response.next

        return all_results[:limit]

    async def fetch_papers(
        self,
        paper_ids: list[str],
    ) -> list[PaperDetails]:
        """
        Fetch full paper details including open access PDF URLs.

        Uses Semantic Scholar /paper/batch endpoint.
        """
        self._ensure_entered()

        if not paper_ids:
            return []

        all_results: list[PaperDetails] = []

        # Process in batches of 500 (API limit)
        batch_size = 500
        for i in range(0, len(paper_ids), batch_size):
            batch_ids = paper_ids[i : i + batch_size]
            response_data = await self._client.get_paper_batch(batch_ids)

            for paper_data in response_data:
                if paper_data:  # API returns null for not found papers
                    all_results.append(PaperDetails.model_validate(paper_data))

        return all_results

    def _get_pdf_url(self, paper: PaperDetails) -> str | None:
        """Get PDF URL, trying open access first, then arXiv fallback."""
        if paper.open_access_pdf and paper.open_access_pdf.url:
            return paper.open_access_pdf.url

        # arXiv fallback: https://arxiv.org/pdf/{arxiv_id}.pdf
        if paper.external_ids and paper.external_ids.get("ArXiv"):
            return f"https://arxiv.org/pdf/{paper.external_ids['ArXiv']}.pdf"

        return None

    async def fetch_papers_with_text(
        self,
        paper_ids: list[str],
    ) -> list[PaperDetails]:
        """
        Fetch full paper details including extracted full text from PDFs.

        Tries open access PDFs first, falls back to arXiv if available.
        Papers without accessible PDFs will have full_text=None.
        """
        self._ensure_entered()

        # First fetch paper details (includes openAccessPdf and externalIds)
        papers = await self.fetch_papers(paper_ids)

        # Extract text for each paper
        for paper in papers:
            pdf_url = self._get_pdf_url(paper)
            if pdf_url:
                try:
                    logger.info(f"Extracting text from: {pdf_url}")
                    paper.full_text = await self.extract_text(pdf_url)
                except Exception as e:
                    logger.warning(f"Failed to extract text for {paper.paper_id}: {e}")
                    paper.full_text = None
            else:
                logger.info(f"No PDF available for {paper.paper_id}")

        return papers

    async def extract_text(self, pdf_url: str) -> str:
        """
        Download PDF and extract text content.

        Uses httpx for download + PyMuPDF for extraction.
        """
        self._ensure_entered()
        import fitz  # PyMuPDF

        # Download PDF
        pdf_bytes = await self._client.download_pdf(pdf_url)

        # Save to temp file and extract text
        with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as f:
            f.write(pdf_bytes)
            temp_path = Path(f.name)

        try:
            # Open PDF and extract text
            doc = fitz.open(temp_path)
            text_parts: list[str] = []

            for page in doc:
                text_parts.append(page.get_text())

            doc.close()
            return "\n".join(text_parts)

        finally:
            # Clean up temp file
            temp_path.unlink(missing_ok=True)
